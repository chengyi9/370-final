{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Alcohol Consumption\n",
    "## Eric Lin, Naveen Janarthanan, Estelle Jiang, Nuo Chen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view imported code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "from IPython.display import HTML\n",
    "\n",
    "from statistical_analysis import df_nice, regression, regression_aft_removing, df\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "accuracy_df = pd.read_csv(\"accuracy_df.csv\")\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view imported code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "> An issue that persists in modern day is abusive alcohol consumption by adolescents. These adolescents tend to start drinking at a very young age for various physical, emotional, and lifestyle changes. Puberty and learning how to live independently often contribute to the commence of alcohol consumption. However, due to the immature mindset that most adolescents have during these early ages, they tend to make bad decisions regarding anything they might term as _\"risky\"_ or _\"cool\"_, such as consuming large amounts of alcohol to get drunk. In fact, **51%** of junior and senior high school students have had at least one drink within the past year and **8 million students drink weekly**. **More than 3 million students drink alone**, **more than 4 million drink when they are upset**, and **less than 3 million drink because they are bored**. In addition, parents, friends, and alcoholic beverage advertisements influences students’ attitudes about alcohol. Students' drinking habit is often heavily influenced by their surroundings and is likely impacted by the local student culture and norms. As such, we wanted to analyze this issue in further detail by **analyzing all the possible variables that could potentially have an effect on student alcohol consumption**, such as personal statistics, parent statistics and education values, and **produce a model to help predict student drinking rates based on these features**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We obtained this Student Alcohol Consumption dataset from Kaggle. The data was attained in a **survey by students of ages 15-22** in regards to their math and/or portuguese language courses during their secondary school education at Gabriel Pereira or Mousinho da Silveira. In the data set, there are **more than thirty features about the student**, such as the gender, the age, as well as whether the student is engaged in a romantic relationship ('Social Index' and 'Drinking Index' in the table below are variables we created, further explained in the 'New Variables' section). The website provided us with two data sets: one dataset contained **students from a math course**, and the other dataset contained **students from a Portuguese language course**. We will be using both of these datasets to **predict the student weekly alcohol consumption rates**. A sample of the dataset features can be seen below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Course</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Walc</th>\n",
       "      <th>absences</th>\n",
       "      <th>activities</th>\n",
       "      <th>address</th>\n",
       "      <th>age</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>famsize</th>\n",
       "      <th>famsup</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>guardian</th>\n",
       "      <th>health</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>nursery</th>\n",
       "      <th>paid</th>\n",
       "      <th>reason</th>\n",
       "      <th>romantic</th>\n",
       "      <th>school</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>sex</th>\n",
       "      <th>studytime</th>\n",
       "      <th>traveltime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Math</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>teacher</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>U</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>GT3</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>mother</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>course</td>\n",
       "      <td>no</td>\n",
       "      <td>GP</td>\n",
       "      <td>yes</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Math</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>U</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>GT3</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>father</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>course</td>\n",
       "      <td>no</td>\n",
       "      <td>GP</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Math</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>no</td>\n",
       "      <td>U</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>LE3</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>mother</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>other</td>\n",
       "      <td>no</td>\n",
       "      <td>GP</td>\n",
       "      <td>yes</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Math</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>services</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>health</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>U</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>GT3</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>mother</td>\n",
       "      <td>5</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>home</td>\n",
       "      <td>yes</td>\n",
       "      <td>GP</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Math</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>U</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>GT3</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>father</td>\n",
       "      <td>5</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>home</td>\n",
       "      <td>no</td>\n",
       "      <td>GP</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Course  Dalc  Fedu      Fjob  G1  G2  G3  Medu     Mjob Pstatus  Walc  \\\n",
       "0   Math     1     4   teacher   5   6   6     4  at_home       A     1   \n",
       "1   Math     1     1     other   5   5   6     1  at_home       T     1   \n",
       "2   Math     2     1     other   7   8  10     1  at_home       T     3   \n",
       "3   Math     1     2  services  15  14  15     4   health       T     1   \n",
       "4   Math     1     3     other   6  10  10     3    other       T     2   \n",
       "\n",
       "   absences activities address  age  failures  famrel famsize famsup  \\\n",
       "0         6         no       U   18         0       4     GT3     no   \n",
       "1         4         no       U   17         0       5     GT3    yes   \n",
       "2        10         no       U   15         3       4     LE3     no   \n",
       "3         2        yes       U   15         0       3     GT3    yes   \n",
       "4         4         no       U   16         0       4     GT3    yes   \n",
       "\n",
       "   freetime  goout guardian  health higher internet nursery paid  reason  \\\n",
       "0         3      4   mother       3    yes       no     yes   no  course   \n",
       "1         3      3   father       3    yes      yes      no   no  course   \n",
       "2         3      2   mother       3    yes      yes     yes  yes   other   \n",
       "3         2      2   mother       5    yes      yes     yes  yes    home   \n",
       "4         3      2   father       5    yes       no     yes  yes    home   \n",
       "\n",
       "  romantic school schoolsup sex  studytime  traveltime  \n",
       "0       no     GP       yes   F          2           2  \n",
       "1       no     GP        no   F          2           1  \n",
       "2       no     GP       yes   F          2           1  \n",
       "3      yes     GP        no   F          3           1  \n",
       "4       no     GP        no   F          2           1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **school** - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\n",
    "- **sex** - student's sex (binary: 'F' - female or 'M' - male)\n",
    "- **age** - student's age (numeric: from 15 to 22)\n",
    "- **address** - student's home address type (binary: 'U' - urban or 'R' - rural)\n",
    "- **famsize** - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n",
    "- **Pstatus** - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n",
    "- **Medu** - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 secondary education or 4 – higher education)\n",
    "- **Fedu** - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 secondary education or 4 – higher education)\n",
    "- **Mjob** - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
    "- **Fjob** - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
    "- **reason** - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n",
    "- **guardian** - student's guardian (nominal: 'mother', 'father' or 'other')\n",
    "- **traveltime** - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "- **studytime** - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
    "- **failures** - number of past class failures (numeric: n if 1<=n<3, else 4)\n",
    "- **schoolsup** - extra educational support (binary: yes or no)\n",
    "- **famsup** - family educational support (binary: yes or no)\n",
    "- **paid** - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "- **activities** - extra-curricular activities (binary: yes or no)\n",
    "- **nursery** - attended nursery school (binary: yes or no)\n",
    "- **higher** - wants to take higher education (binary: yes or no)\n",
    "- **internet** - Internet access at home (binary: yes or no)\n",
    "- **romantic** - with a romantic relationship (binary: yes or no)\n",
    "- **famrel** - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "- **freetime** - free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "- **goout** - going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "- **Dalc** - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "- **Walc** - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "- **health** - current health status (numeric: from 1 - very bad to 5 - very good)\n",
    "- **absences** - number of school absences (numeric: from 0 to 93)\n",
    "\n",
    "These grades are related with the course subject, Math or Portuguese:\n",
    "- **G1** - first period grade (numeric: from 0 to 20)\n",
    "- **G2** - second period grade (numeric: from 0 to 20)\n",
    "- **G3** - final grade (numeric: from 0 to 20, output target)\n",
    "\n",
    "Descriptions from https://www.kaggle.com/uciml/student-alcohol-consumption#student-por.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "> Upon analyzing both datasets, we were glad to find that there were **no missing information**. As such, we did not need to worry about any significant data manipulation of the actual data obtained. The only data manipulation we had to perform was **combining the 2 datasets**: the math course dataset and the Portugese language course dataset. Because we were analyzing the rates of drinking based on various students' personal features, and since we came to a consensus that the **type of course would _NOT_ have a significant impact on the the amout a student drinks**, we combined the rows in both the dataframes to create a large dataframe (**1044 rows**) so that we have **more data to determine a stronger model to predict the student weekly drinking rates**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Variables\n",
    "#### Drinking per Week Index (DWI)\n",
    "> The **Drinking per Week Index (DWI) is an index that measures the amount of alcohol a student consumes throughout the entire week**. From the dataset given, we wanted a way of measuring alcohol consumption rates in the entire week. As such, the best way to go about this was to combine both weekday and weekend indexes and find the average of these indexes. However, since both indexes are weighted differently, we had to take that into account when finding the average of the weekday and weekend alcohol consumption index by multiplying each index by the number of days there are  in the given variable divided by the number of days there are in a week (as shown below). This **DWI ranges from one to five, where five represents high alcohol consumption and one represents low alcohol consumption**. Ultimately, the DWI allows us to convert categorical variables into a continuous variable, which we can use to run a statisticsl regression model and make predictions about how much alcohol a student consumes in the entire week. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "Drinking~per~Week~Index = \\bigg[ \\dfrac{ \\big(Weekend~Alc~Consumption \\times 5\\big) + \\big(Weekday~Alc~Consumption \\times 2\\big) }{7}  \\bigg]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social Index\n",
    "> A study done by healthtalk on drugs and alcohol suggests that **most high school and college students drink to socialize with others, to have fun and relax**. When junior/senior high school students and college students go out with their friends, it is usually to grab some drinks and attend a party. Students tend to drink at abnormally high raters their first year of college when they move out of the house. Students with access to the internet tend to utilize social media at high rates, and as such are closely associated with how social students are. Additionally, whether a student is in a relationship or not strongly dictates how social they are, as relationships usually mandate meeting your partner's friends, hanging out with your partnet often, and attending various events with your partner and your/their friends. Engaging in extra-curriculat activities has a negative correlation with respect to how social a student is, as a student entertained in more extra-curricular activities will likely _NOT_ have time to socialize with others. As such, **we decided to create the Social Index feature, which indicates how social a given student is based on how often they go out with friends, whether they have access to the internet (1=yes, 0=no), whether they are in a romantic relationship (1=yes, 0=no), and whether they are doing any extra-curricular activities (1=yes, 0=no).** We created the social index formula based on how each feature correlates with the the DWI. Below is the formula to determine the Social Index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "Social~Index = \\big(0.25 \\times Go~Out~w/~Friends \\big) + \\big(0.02 \\times Internet \\big) + \\big(0.03 \\times In~a~Romantic~Relationship \\big) + \\big(\\text{-} 0.01 \\times Extra~Curricular~Activities \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (the statistical analysis stuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Studnets' Social Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/plt_hist_social_index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The distribution of students' social index graph (illustrated above) allows us to visually see how social most students in the dataset are. **A lower social index indicates lower social interactions and thus, a less social student. On the contrary, a higher social index indicates many more social interactions with others likely from going out often with friends, a more social student.** The graph illustrates that most students are social to some extent, with very few people being completely anti-social (as there are only ~100 students with a social index less than 0.5. The remaining are somewhat social (with an index of 0.5 - 1.0) or very social (with an index of 1.2 to 1.29), which makes sense since most students as this age tend to be very social to gain popularity. We believe that **this desire to be popular has a strong correlation with drinking**, as most \"cool\" people partake in some illegal activities like underage drinking, which a student who would want to be deemed as \"cool\" and be a \"popular kid\" would have to engage in these illegal activities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age Distribution in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Naveen/anaconda3/lib/python3.7/site-packages/IPython/core/display.py:689: UserWarning:\n",
      "\n",
      "Consider using IPython.display.IFrame instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~JYCestelle/4.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Age Distribution - plotly\n",
    "data = [go.Bar(\n",
    "            x= df['age'].value_counts(),\n",
    "            y= df['age'].value_counts().index,\n",
    "            marker=dict(\n",
    "                color='rgba(122, 120, 168, 0.8)',\n",
    "            ),\n",
    "            orientation = 'h'\n",
    ")]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Age Distribution',\n",
    "    xaxis=dict(\n",
    "        title='Occurrence of particular age'),\n",
    "    yaxis=dict(\n",
    "        title='Age')\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='horizontal-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since our dataset is about the drinking behaviors of students, we are curious about the age distribution of our dataset. Therefore, we count the occurrence of each age we have on the dataset and show the results by drawing a horizontal bar chart. We found **the majority of our users are around 15 - 18 years old** and there are a few notable outliers here, with students as old as 21-22. Based on the information of our dataset, the students who took the survey came from secondary school, therefore, the age distribution makes lots of sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alcohol Consumption Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Naveen/anaconda3/lib/python3.7/site-packages/IPython/core/display.py:689: UserWarning:\n",
      "\n",
      "Consider using IPython.display.IFrame instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~JYCestelle/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dr = df['Dalc'].value_counts().tolist()\n",
    "wr = df['Walc'].value_counts().tolist()\n",
    "\n",
    "# draw graph by using plotly\n",
    "trace0 = go.Bar(\n",
    "    x = df['Dalc'].value_counts().index.tolist(),\n",
    "    y = dr,\n",
    "    name='Weekday Alcohol Consumption',\n",
    "    marker=dict(\n",
    "        color='rgb(49,130,189)'\n",
    "    )\n",
    ")\n",
    "trace1 = go.Bar(\n",
    "    x = df['Walc'].value_counts().index,\n",
    "    y = wr,\n",
    "    name='Weekends Alcohol Consumption',\n",
    "    marker=dict(\n",
    "        color='rgb(204,204,204)',\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1]\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(tickangle=-45),\n",
    "    barmode='group',   \n",
    "    title='Weekday Alcohol Consumption VS Weekends Alcohol Consumption',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='angled-text-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In our dataset, we can see each student's alcohol consumption for weekdays and weekends. Therefore, we think it will be interesting to compare the number of students' alcohol consumption level both in weekdays and weekends. Based on the graph, we can tell that most of students will not drink on weekdays (level 1-2) and also on weekends. But there are more students drink a lot(level5) on weekends compare to students drink a lot on week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between the Social Index and the DWI (the 2 New Variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/plt_scatter_social_dwi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The scatter plot above represents the correlation between the social index and the drinking per week index, both features that we created. Ultimately, **we wanted to see whether there was a correlation between the social index and the variable we are predicting in the statistical analysis: the DWI.** From the scatter plot, there seems to be a positive linear correlation, such that **as a student's social index increases their weekly drinking consumption rate increases**. This is a low-level method of verifying that the feature we created will in-fact have some significance with our desired output before we compute regression models on the enitre dataset with these new variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>drinking</td>     <th>  R-squared:         </th> <td>   0.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   10.07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 12 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>5.39e-70</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:53:57</td>     <th>  Log-Likelihood:    </th> <td> -1320.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1044</td>      <th>  AIC:               </th> <td>   2768.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   980</td>      <th>  BIC:               </th> <td>   3085.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    63</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>              <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>            <td>    0.0987</td> <td>    0.690</td> <td>    0.143</td> <td> 0.886</td> <td>   -1.256</td> <td>    1.453</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>school[T.MS]</th>         <td>   -0.0349</td> <td>    0.080</td> <td>   -0.437</td> <td> 0.663</td> <td>   -0.192</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex[T.M]</th>             <td>    0.5601</td> <td>    0.064</td> <td>    8.754</td> <td> 0.000</td> <td>    0.435</td> <td>    0.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>address[T.U]</th>         <td>   -0.1000</td> <td>    0.072</td> <td>   -1.387</td> <td> 0.166</td> <td>   -0.242</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pstatus[T.T]</th>         <td>    0.0563</td> <td>    0.094</td> <td>    0.597</td> <td> 0.551</td> <td>   -0.129</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famsize[T.LE3]</th>       <td>    0.1804</td> <td>    0.065</td> <td>    2.776</td> <td> 0.006</td> <td>    0.053</td> <td>    0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Medu[T.1]</th>            <td>   -0.5001</td> <td>    0.316</td> <td>   -1.584</td> <td> 0.114</td> <td>   -1.120</td> <td>    0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Medu[T.2]</th>            <td>   -0.7287</td> <td>    0.317</td> <td>   -2.302</td> <td> 0.022</td> <td>   -1.350</td> <td>   -0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Medu[T.3]</th>            <td>   -0.5355</td> <td>    0.321</td> <td>   -1.670</td> <td> 0.095</td> <td>   -1.165</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Medu[T.4]</th>            <td>   -0.7043</td> <td>    0.330</td> <td>   -2.135</td> <td> 0.033</td> <td>   -1.352</td> <td>   -0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fedu[T.1]</th>            <td>    0.8912</td> <td>    0.313</td> <td>    2.845</td> <td> 0.005</td> <td>    0.276</td> <td>    1.506</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fedu[T.2]</th>            <td>    0.8991</td> <td>    0.316</td> <td>    2.847</td> <td> 0.004</td> <td>    0.279</td> <td>    1.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fedu[T.3]</th>            <td>    0.9227</td> <td>    0.319</td> <td>    2.893</td> <td> 0.004</td> <td>    0.297</td> <td>    1.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fedu[T.4]</th>            <td>    1.0224</td> <td>    0.326</td> <td>    3.133</td> <td> 0.002</td> <td>    0.382</td> <td>    1.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob[T.health]</th>       <td>   -0.1158</td> <td>    0.146</td> <td>   -0.792</td> <td> 0.429</td> <td>   -0.403</td> <td>    0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob[T.other]</th>        <td>   -0.1194</td> <td>    0.085</td> <td>   -1.397</td> <td> 0.163</td> <td>   -0.287</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob[T.services]</th>     <td>   -0.0809</td> <td>    0.101</td> <td>   -0.797</td> <td> 0.425</td> <td>   -0.280</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mjob[T.teacher]</th>      <td>    0.0839</td> <td>    0.139</td> <td>    0.604</td> <td> 0.546</td> <td>   -0.188</td> <td>    0.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob[T.health]</th>       <td>    0.0744</td> <td>    0.197</td> <td>    0.379</td> <td> 0.705</td> <td>   -0.311</td> <td>    0.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob[T.other]</th>        <td>    0.2393</td> <td>    0.126</td> <td>    1.895</td> <td> 0.058</td> <td>   -0.009</td> <td>    0.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob[T.services]</th>     <td>    0.4353</td> <td>    0.132</td> <td>    3.298</td> <td> 0.001</td> <td>    0.176</td> <td>    0.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob[T.teacher]</th>      <td>   -0.1507</td> <td>    0.181</td> <td>   -0.834</td> <td> 0.405</td> <td>   -0.505</td> <td>    0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason[T.home]</th>       <td>    0.0597</td> <td>    0.075</td> <td>    0.800</td> <td> 0.424</td> <td>   -0.087</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason[T.other]</th>      <td>    0.2665</td> <td>    0.100</td> <td>    2.674</td> <td> 0.008</td> <td>    0.071</td> <td>    0.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason[T.reputation]</th> <td>    0.1025</td> <td>    0.078</td> <td>    1.317</td> <td> 0.188</td> <td>   -0.050</td> <td>    0.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>guardian[T.mother]</th>   <td>   -0.1167</td> <td>    0.072</td> <td>   -1.630</td> <td> 0.103</td> <td>   -0.257</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>guardian[T.other]</th>    <td>   -0.3010</td> <td>    0.136</td> <td>   -2.213</td> <td> 0.027</td> <td>   -0.568</td> <td>   -0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>traveltime[T.2]</th>      <td>   -0.1055</td> <td>    0.066</td> <td>   -1.587</td> <td> 0.113</td> <td>   -0.236</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>traveltime[T.3]</th>      <td>    0.0769</td> <td>    0.119</td> <td>    0.646</td> <td> 0.518</td> <td>   -0.157</td> <td>    0.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>traveltime[T.4]</th>      <td>    0.6269</td> <td>    0.199</td> <td>    3.145</td> <td> 0.002</td> <td>    0.236</td> <td>    1.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>studytime[T.2]</th>       <td>   -0.2385</td> <td>    0.071</td> <td>   -3.355</td> <td> 0.001</td> <td>   -0.378</td> <td>   -0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>studytime[T.3]</th>       <td>   -0.2966</td> <td>    0.099</td> <td>   -3.005</td> <td> 0.003</td> <td>   -0.490</td> <td>   -0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>studytime[T.4]</th>       <td>   -0.3700</td> <td>    0.136</td> <td>   -2.725</td> <td> 0.007</td> <td>   -0.636</td> <td>   -0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>schoolsup[T.yes]</th>     <td>   -0.0173</td> <td>    0.095</td> <td>   -0.182</td> <td> 0.856</td> <td>   -0.204</td> <td>    0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famsup[T.yes]</th>        <td>   -0.0406</td> <td>    0.062</td> <td>   -0.660</td> <td> 0.509</td> <td>   -0.161</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>paid[T.yes]</th>          <td>    0.2707</td> <td>    0.081</td> <td>    3.331</td> <td> 0.001</td> <td>    0.111</td> <td>    0.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>activities[T.1]</th>      <td>   -0.0571</td> <td>    0.059</td> <td>   -0.969</td> <td> 0.333</td> <td>   -0.173</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nursery[T.yes]</th>       <td>   -0.1978</td> <td>    0.073</td> <td>   -2.719</td> <td> 0.007</td> <td>   -0.341</td> <td>   -0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>higher[T.yes]</th>        <td>    0.0830</td> <td>    0.113</td> <td>    0.736</td> <td> 0.462</td> <td>   -0.138</td> <td>    0.304</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>internet[T.1]</th>        <td>    0.0354</td> <td>    0.076</td> <td>    0.463</td> <td> 0.644</td> <td>   -0.115</td> <td>    0.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>romantic[T.1]</th>        <td>    0.0066</td> <td>    0.063</td> <td>    0.106</td> <td> 0.915</td> <td>   -0.116</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famrel[T.2]</th>          <td>   -0.3059</td> <td>    0.220</td> <td>   -1.393</td> <td> 0.164</td> <td>   -0.737</td> <td>    0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famrel[T.3]</th>          <td>   -0.3290</td> <td>    0.186</td> <td>   -1.765</td> <td> 0.078</td> <td>   -0.695</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famrel[T.4]</th>          <td>   -0.5445</td> <td>    0.177</td> <td>   -3.070</td> <td> 0.002</td> <td>   -0.893</td> <td>   -0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famrel[T.5]</th>          <td>   -0.7811</td> <td>    0.180</td> <td>   -4.337</td> <td> 0.000</td> <td>   -1.134</td> <td>   -0.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>freetime[T.2]</th>        <td>   -0.0155</td> <td>    0.135</td> <td>   -0.115</td> <td> 0.909</td> <td>   -0.280</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>freetime[T.3]</th>        <td>   -0.0284</td> <td>    0.125</td> <td>   -0.226</td> <td> 0.821</td> <td>   -0.274</td> <td>    0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>freetime[T.4]</th>        <td>   -0.0147</td> <td>    0.132</td> <td>   -0.111</td> <td> 0.911</td> <td>   -0.274</td> <td>    0.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>freetime[T.5]</th>        <td>   -0.0397</td> <td>    0.152</td> <td>   -0.261</td> <td> 0.794</td> <td>   -0.339</td> <td>    0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goout[T.2]</th>           <td>   -0.0391</td> <td>    0.109</td> <td>   -0.360</td> <td> 0.719</td> <td>   -0.252</td> <td>    0.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goout[T.3]</th>           <td>    0.0552</td> <td>    0.089</td> <td>    0.618</td> <td> 0.537</td> <td>   -0.120</td> <td>    0.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goout[T.4]</th>           <td>    0.2783</td> <td>    0.081</td> <td>    3.418</td> <td> 0.001</td> <td>    0.119</td> <td>    0.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goout[T.5]</th>           <td>    0.5198</td> <td>    0.081</td> <td>    6.388</td> <td> 0.000</td> <td>    0.360</td> <td>    0.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health[T.2]</th>          <td>    0.1730</td> <td>    0.116</td> <td>    1.496</td> <td> 0.135</td> <td>   -0.054</td> <td>    0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health[T.3]</th>          <td>    0.1748</td> <td>    0.103</td> <td>    1.694</td> <td> 0.091</td> <td>   -0.028</td> <td>    0.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health[T.4]</th>          <td>    0.2516</td> <td>    0.107</td> <td>    2.349</td> <td> 0.019</td> <td>    0.041</td> <td>    0.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health[T.5]</th>          <td>    0.3050</td> <td>    0.095</td> <td>    3.211</td> <td> 0.001</td> <td>    0.119</td> <td>    0.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Course[T.Por]</th>        <td>    0.1370</td> <td>    0.070</td> <td>    1.946</td> <td> 0.052</td> <td>   -0.001</td> <td>    0.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>                  <td>    0.0641</td> <td>    0.027</td> <td>    2.374</td> <td> 0.018</td> <td>    0.011</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>failures</th>             <td>    0.0096</td> <td>    0.051</td> <td>    0.190</td> <td> 0.849</td> <td>   -0.090</td> <td>    0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>absences</th>             <td>    0.0193</td> <td>    0.005</td> <td>    3.947</td> <td> 0.000</td> <td>    0.010</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>G1</th>                   <td>   -0.0254</td> <td>    0.019</td> <td>   -1.321</td> <td> 0.187</td> <td>   -0.063</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>G2</th>                   <td>    0.0076</td> <td>    0.024</td> <td>    0.315</td> <td> 0.753</td> <td>   -0.040</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>G3</th>                   <td>    0.0026</td> <td>    0.018</td> <td>    0.144</td> <td> 0.885</td> <td>   -0.033</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>index</th>                <td>    0.7725</td> <td>    0.112</td> <td>    6.895</td> <td> 0.000</td> <td>    0.553</td> <td>    0.992</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>19.689</td> <th>  Durbin-Watson:     </th> <td>   1.915</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  20.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.326</td> <th>  Prob(JB):          </th> <td>3.86e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.205</td> <th>  Cond. No.          </th> <td>9.68e+15</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 7.89e-27. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               drinking   R-squared:                       0.393\n",
       "Model:                            OLS   Adj. R-squared:                  0.354\n",
       "Method:                 Least Squares   F-statistic:                     10.07\n",
       "Date:                Tue, 12 Mar 2019   Prob (F-statistic):           5.39e-70\n",
       "Time:                        15:53:57   Log-Likelihood:                -1320.1\n",
       "No. Observations:                1044   AIC:                             2768.\n",
       "Df Residuals:                     980   BIC:                             3085.\n",
       "Df Model:                          63                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "========================================================================================\n",
       "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------\n",
       "Intercept                0.0987      0.690      0.143      0.886      -1.256       1.453\n",
       "school[T.MS]            -0.0349      0.080     -0.437      0.663      -0.192       0.122\n",
       "sex[T.M]                 0.5601      0.064      8.754      0.000       0.435       0.686\n",
       "address[T.U]            -0.1000      0.072     -1.387      0.166      -0.242       0.041\n",
       "Pstatus[T.T]             0.0563      0.094      0.597      0.551      -0.129       0.242\n",
       "famsize[T.LE3]           0.1804      0.065      2.776      0.006       0.053       0.308\n",
       "Medu[T.1]               -0.5001      0.316     -1.584      0.114      -1.120       0.120\n",
       "Medu[T.2]               -0.7287      0.317     -2.302      0.022      -1.350      -0.107\n",
       "Medu[T.3]               -0.5355      0.321     -1.670      0.095      -1.165       0.094\n",
       "Medu[T.4]               -0.7043      0.330     -2.135      0.033      -1.352      -0.057\n",
       "Fedu[T.1]                0.8912      0.313      2.845      0.005       0.276       1.506\n",
       "Fedu[T.2]                0.8991      0.316      2.847      0.004       0.279       1.519\n",
       "Fedu[T.3]                0.9227      0.319      2.893      0.004       0.297       1.549\n",
       "Fedu[T.4]                1.0224      0.326      3.133      0.002       0.382       1.663\n",
       "Mjob[T.health]          -0.1158      0.146     -0.792      0.429      -0.403       0.171\n",
       "Mjob[T.other]           -0.1194      0.085     -1.397      0.163      -0.287       0.048\n",
       "Mjob[T.services]        -0.0809      0.101     -0.797      0.425      -0.280       0.118\n",
       "Mjob[T.teacher]          0.0839      0.139      0.604      0.546      -0.188       0.356\n",
       "Fjob[T.health]           0.0744      0.197      0.379      0.705      -0.311       0.460\n",
       "Fjob[T.other]            0.2393      0.126      1.895      0.058      -0.009       0.487\n",
       "Fjob[T.services]         0.4353      0.132      3.298      0.001       0.176       0.694\n",
       "Fjob[T.teacher]         -0.1507      0.181     -0.834      0.405      -0.505       0.204\n",
       "reason[T.home]           0.0597      0.075      0.800      0.424      -0.087       0.206\n",
       "reason[T.other]          0.2665      0.100      2.674      0.008       0.071       0.462\n",
       "reason[T.reputation]     0.1025      0.078      1.317      0.188      -0.050       0.255\n",
       "guardian[T.mother]      -0.1167      0.072     -1.630      0.103      -0.257       0.024\n",
       "guardian[T.other]       -0.3010      0.136     -2.213      0.027      -0.568      -0.034\n",
       "traveltime[T.2]         -0.1055      0.066     -1.587      0.113      -0.236       0.025\n",
       "traveltime[T.3]          0.0769      0.119      0.646      0.518      -0.157       0.311\n",
       "traveltime[T.4]          0.6269      0.199      3.145      0.002       0.236       1.018\n",
       "studytime[T.2]          -0.2385      0.071     -3.355      0.001      -0.378      -0.099\n",
       "studytime[T.3]          -0.2966      0.099     -3.005      0.003      -0.490      -0.103\n",
       "studytime[T.4]          -0.3700      0.136     -2.725      0.007      -0.636      -0.104\n",
       "schoolsup[T.yes]        -0.0173      0.095     -0.182      0.856      -0.204       0.169\n",
       "famsup[T.yes]           -0.0406      0.062     -0.660      0.509      -0.161       0.080\n",
       "paid[T.yes]              0.2707      0.081      3.331      0.001       0.111       0.430\n",
       "activities[T.1]         -0.0571      0.059     -0.969      0.333      -0.173       0.059\n",
       "nursery[T.yes]          -0.1978      0.073     -2.719      0.007      -0.341      -0.055\n",
       "higher[T.yes]            0.0830      0.113      0.736      0.462      -0.138       0.304\n",
       "internet[T.1]            0.0354      0.076      0.463      0.644      -0.115       0.185\n",
       "romantic[T.1]            0.0066      0.063      0.106      0.915      -0.116       0.129\n",
       "famrel[T.2]             -0.3059      0.220     -1.393      0.164      -0.737       0.125\n",
       "famrel[T.3]             -0.3290      0.186     -1.765      0.078      -0.695       0.037\n",
       "famrel[T.4]             -0.5445      0.177     -3.070      0.002      -0.893      -0.196\n",
       "famrel[T.5]             -0.7811      0.180     -4.337      0.000      -1.134      -0.428\n",
       "freetime[T.2]           -0.0155      0.135     -0.115      0.909      -0.280       0.249\n",
       "freetime[T.3]           -0.0284      0.125     -0.226      0.821      -0.274       0.218\n",
       "freetime[T.4]           -0.0147      0.132     -0.111      0.911      -0.274       0.244\n",
       "freetime[T.5]           -0.0397      0.152     -0.261      0.794      -0.339       0.259\n",
       "goout[T.2]              -0.0391      0.109     -0.360      0.719      -0.252       0.174\n",
       "goout[T.3]               0.0552      0.089      0.618      0.537      -0.120       0.231\n",
       "goout[T.4]               0.2783      0.081      3.418      0.001       0.119       0.438\n",
       "goout[T.5]               0.5198      0.081      6.388      0.000       0.360       0.679\n",
       "health[T.2]              0.1730      0.116      1.496      0.135      -0.054       0.400\n",
       "health[T.3]              0.1748      0.103      1.694      0.091      -0.028       0.377\n",
       "health[T.4]              0.2516      0.107      2.349      0.019       0.041       0.462\n",
       "health[T.5]              0.3050      0.095      3.211      0.001       0.119       0.491\n",
       "Course[T.Por]            0.1370      0.070      1.946      0.052      -0.001       0.275\n",
       "age                      0.0641      0.027      2.374      0.018       0.011       0.117\n",
       "failures                 0.0096      0.051      0.190      0.849      -0.090       0.109\n",
       "absences                 0.0193      0.005      3.947      0.000       0.010       0.029\n",
       "G1                      -0.0254      0.019     -1.321      0.187      -0.063       0.012\n",
       "G2                       0.0076      0.024      0.315      0.753      -0.040       0.055\n",
       "G3                       0.0026      0.018      0.144      0.885      -0.033       0.039\n",
       "index                    0.7725      0.112      6.895      0.000       0.553       0.992\n",
       "==============================================================================\n",
       "Omnibus:                       19.689   Durbin-Watson:                   1.915\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               20.322\n",
       "Skew:                           0.326   Prob(JB):                     3.86e-05\n",
       "Kurtosis:                       3.205   Cond. No.                     9.68e+15\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 7.89e-27. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above regression summary provides the regression statistics about how each variable correlates with the DWI. We will use this multi-linear regression to predict the DWI of students based on the dataset given. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual DWI vs. Predicted DWI (first analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/plt_actual_pred_dwi_before.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After determining the predicted values of the DWI based on the regression model, we decided to plot it against the actual outcome to see how well the model predicts the DWI. Ideally, **the data points should lie on the red line, as any data on the red line illustrates an exact match of DWI between the predicted and actual data**. However, its is quite clear that **most of the data points do _NOT_ lie on the red line. This is most likely due to the dataset having many features that do _NOT_ have a strong impact on DWI**, as depicted by a high p-value score. As such, we decided **there are clearly some features we should remove and re-run the multi-linear regression to generate a more accurate regression model**. However, before removing features only based on their p-values, we wanted to see how the continuous variables correlate with one another in this dataset. <br/>  (**See 'Challenges' section for why we did not use feature selection methods**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of Continuous Variables from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/plt_heat_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The heatmap above captures the covariance between continuous variables**. We created this map to **observe the correlation between the drinking index and other variables**. It it worth noting that we did _NOT_ include any categorical varibable, as we want to keep different types of variable seperated. Before creating the heatmap, **we set an arbitary threshold of 0.1**. In other words, we will be eliminating variables from the model if we do not observe significant correlation (correlation below 0.1) between the variables to prevent overfitting the model. Nonetheless, after drawing out the heatmap, every variable has a covariance higher than 0.1, so **we kept all the continuous variables in the regression model**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Summary (after removing insignificant variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>drinking</td>     <th>  R-squared:         </th> <td>   0.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.355</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   14.67</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 12 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>1.41e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:41:22</td>     <th>  Log-Likelihood:    </th> <td> -1330.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1044</td>      <th>  AIC:               </th> <td>   2746.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1001</td>      <th>  BIC:               </th> <td>   2959.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    42</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>              <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>            <td>   -0.4934</td> <td>    0.760</td> <td>   -0.649</td> <td> 0.516</td> <td>   -1.985</td> <td>    0.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex[T.M]</th>             <td>    0.5831</td> <td>    0.060</td> <td>    9.659</td> <td> 0.000</td> <td>    0.465</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Medu[T.1]</th>            <td>   -0.5075</td> <td>    0.313</td> <td>   -1.623</td> <td> 0.105</td> <td>   -1.121</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Medu[T.2]</th>            <td>   -0.7835</td> <td>    0.313</td> <td>   -2.504</td> <td> 0.012</td> <td>   -1.398</td> <td>   -0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Medu[T.3]</th>            <td>   -0.5892</td> <td>    0.316</td> <td>   -1.866</td> <td> 0.062</td> <td>   -1.209</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Medu[T.4]</th>            <td>   -0.7153</td> <td>    0.319</td> <td>   -2.243</td> <td> 0.025</td> <td>   -1.341</td> <td>   -0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fedu[T.1]</th>            <td>    0.9742</td> <td>    0.308</td> <td>    3.160</td> <td> 0.002</td> <td>    0.369</td> <td>    1.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fedu[T.2]</th>            <td>    0.9826</td> <td>    0.311</td> <td>    3.163</td> <td> 0.002</td> <td>    0.373</td> <td>    1.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fedu[T.3]</th>            <td>    0.9947</td> <td>    0.315</td> <td>    3.158</td> <td> 0.002</td> <td>    0.377</td> <td>    1.613</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fedu[T.4]</th>            <td>    1.1106</td> <td>    0.322</td> <td>    3.453</td> <td> 0.001</td> <td>    0.479</td> <td>    1.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob[T.health]</th>       <td>    0.0876</td> <td>    0.191</td> <td>    0.459</td> <td> 0.647</td> <td>   -0.287</td> <td>    0.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob[T.other]</th>        <td>    0.2401</td> <td>    0.123</td> <td>    1.957</td> <td> 0.051</td> <td>   -0.001</td> <td>    0.481</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob[T.services]</th>     <td>    0.4474</td> <td>    0.128</td> <td>    3.487</td> <td> 0.001</td> <td>    0.196</td> <td>    0.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fjob[T.teacher]</th>      <td>   -0.1843</td> <td>    0.175</td> <td>   -1.052</td> <td> 0.293</td> <td>   -0.528</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason[T.home]</th>       <td>    0.0405</td> <td>    0.073</td> <td>    0.555</td> <td> 0.579</td> <td>   -0.103</td> <td>    0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason[T.other]</th>      <td>    0.2505</td> <td>    0.098</td> <td>    2.568</td> <td> 0.010</td> <td>    0.059</td> <td>    0.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason[T.reputation]</th> <td>    0.0792</td> <td>    0.075</td> <td>    1.055</td> <td> 0.292</td> <td>   -0.068</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>guardian[T.mother]</th>   <td>   -0.0828</td> <td>    0.069</td> <td>   -1.192</td> <td> 0.234</td> <td>   -0.219</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>guardian[T.other]</th>    <td>   -0.3107</td> <td>    0.131</td> <td>   -2.369</td> <td> 0.018</td> <td>   -0.568</td> <td>   -0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>traveltime[T.2]</th>      <td>   -0.0716</td> <td>    0.064</td> <td>   -1.125</td> <td> 0.261</td> <td>   -0.196</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>traveltime[T.3]</th>      <td>    0.1293</td> <td>    0.113</td> <td>    1.148</td> <td> 0.251</td> <td>   -0.092</td> <td>    0.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>traveltime[T.4]</th>      <td>    0.6905</td> <td>    0.193</td> <td>    3.570</td> <td> 0.000</td> <td>    0.311</td> <td>    1.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>studytime[T.2]</th>       <td>   -0.2419</td> <td>    0.068</td> <td>   -3.557</td> <td> 0.000</td> <td>   -0.375</td> <td>   -0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>studytime[T.3]</th>       <td>   -0.3243</td> <td>    0.094</td> <td>   -3.436</td> <td> 0.001</td> <td>   -0.510</td> <td>   -0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>studytime[T.4]</th>       <td>   -0.3871</td> <td>    0.132</td> <td>   -2.930</td> <td> 0.003</td> <td>   -0.646</td> <td>   -0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>paid[T.yes]</th>          <td>    0.2778</td> <td>    0.079</td> <td>    3.519</td> <td> 0.000</td> <td>    0.123</td> <td>    0.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nursery[T.yes]</th>       <td>   -0.1761</td> <td>    0.072</td> <td>   -2.461</td> <td> 0.014</td> <td>   -0.317</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famrel[T.2]</th>          <td>   -0.2902</td> <td>    0.213</td> <td>   -1.361</td> <td> 0.174</td> <td>   -0.709</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famrel[T.3]</th>          <td>   -0.2921</td> <td>    0.182</td> <td>   -1.608</td> <td> 0.108</td> <td>   -0.649</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famrel[T.4]</th>          <td>   -0.5176</td> <td>    0.172</td> <td>   -3.005</td> <td> 0.003</td> <td>   -0.856</td> <td>   -0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>famrel[T.5]</th>          <td>   -0.7569</td> <td>    0.175</td> <td>   -4.314</td> <td> 0.000</td> <td>   -1.101</td> <td>   -0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goout[T.2]</th>           <td>   -0.1904</td> <td>    0.439</td> <td>   -0.433</td> <td> 0.665</td> <td>   -1.053</td> <td>    0.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goout[T.3]</th>           <td>   -0.2495</td> <td>    0.852</td> <td>   -0.293</td> <td> 0.770</td> <td>   -1.922</td> <td>    1.423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goout[T.4]</th>           <td>   -0.1777</td> <td>    1.268</td> <td>   -0.140</td> <td> 0.889</td> <td>   -2.666</td> <td>    2.310</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goout[T.5]</th>           <td>   -0.1155</td> <td>    1.697</td> <td>   -0.068</td> <td> 0.946</td> <td>   -3.445</td> <td>    3.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health[T.2]</th>          <td>    0.1798</td> <td>    0.114</td> <td>    1.577</td> <td> 0.115</td> <td>   -0.044</td> <td>    0.403</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health[T.3]</th>          <td>    0.1912</td> <td>    0.101</td> <td>    1.889</td> <td> 0.059</td> <td>   -0.007</td> <td>    0.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health[T.4]</th>          <td>    0.2528</td> <td>    0.105</td> <td>    2.404</td> <td> 0.016</td> <td>    0.046</td> <td>    0.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health[T.5]</th>          <td>    0.3066</td> <td>    0.093</td> <td>    3.308</td> <td> 0.001</td> <td>    0.125</td> <td>    0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Course[T.Por]</th>        <td>    0.1268</td> <td>    0.066</td> <td>    1.927</td> <td> 0.054</td> <td>   -0.002</td> <td>    0.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>                  <td>    0.0720</td> <td>    0.025</td> <td>    2.875</td> <td> 0.004</td> <td>    0.023</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>absences</th>             <td>    0.0198</td> <td>    0.005</td> <td>    4.197</td> <td> 0.000</td> <td>    0.011</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>index</th>                <td>    1.3718</td> <td>    1.687</td> <td>    0.813</td> <td> 0.416</td> <td>   -1.938</td> <td>    4.682</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>21.226</td> <th>  Durbin-Watson:     </th> <td>   1.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  21.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.346</td> <th>  Prob(JB):          </th> <td>1.70e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.163</td> <th>  Cond. No.          </th> <td>1.87e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.87e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               drinking   R-squared:                       0.381\n",
       "Model:                            OLS   Adj. R-squared:                  0.355\n",
       "Method:                 Least Squares   F-statistic:                     14.67\n",
       "Date:                Tue, 12 Mar 2019   Prob (F-statistic):           1.41e-77\n",
       "Time:                        19:41:22   Log-Likelihood:                -1330.2\n",
       "No. Observations:                1044   AIC:                             2746.\n",
       "Df Residuals:                    1001   BIC:                             2959.\n",
       "Df Model:                          42                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "========================================================================================\n",
       "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------\n",
       "Intercept               -0.4934      0.760     -0.649      0.516      -1.985       0.999\n",
       "sex[T.M]                 0.5831      0.060      9.659      0.000       0.465       0.702\n",
       "Medu[T.1]               -0.5075      0.313     -1.623      0.105      -1.121       0.106\n",
       "Medu[T.2]               -0.7835      0.313     -2.504      0.012      -1.398      -0.169\n",
       "Medu[T.3]               -0.5892      0.316     -1.866      0.062      -1.209       0.030\n",
       "Medu[T.4]               -0.7153      0.319     -2.243      0.025      -1.341      -0.090\n",
       "Fedu[T.1]                0.9742      0.308      3.160      0.002       0.369       1.579\n",
       "Fedu[T.2]                0.9826      0.311      3.163      0.002       0.373       1.592\n",
       "Fedu[T.3]                0.9947      0.315      3.158      0.002       0.377       1.613\n",
       "Fedu[T.4]                1.1106      0.322      3.453      0.001       0.479       1.742\n",
       "Fjob[T.health]           0.0876      0.191      0.459      0.647      -0.287       0.463\n",
       "Fjob[T.other]            0.2401      0.123      1.957      0.051      -0.001       0.481\n",
       "Fjob[T.services]         0.4474      0.128      3.487      0.001       0.196       0.699\n",
       "Fjob[T.teacher]         -0.1843      0.175     -1.052      0.293      -0.528       0.159\n",
       "reason[T.home]           0.0405      0.073      0.555      0.579      -0.103       0.183\n",
       "reason[T.other]          0.2505      0.098      2.568      0.010       0.059       0.442\n",
       "reason[T.reputation]     0.0792      0.075      1.055      0.292      -0.068       0.226\n",
       "guardian[T.mother]      -0.0828      0.069     -1.192      0.234      -0.219       0.054\n",
       "guardian[T.other]       -0.3107      0.131     -2.369      0.018      -0.568      -0.053\n",
       "traveltime[T.2]         -0.0716      0.064     -1.125      0.261      -0.196       0.053\n",
       "traveltime[T.3]          0.1293      0.113      1.148      0.251      -0.092       0.350\n",
       "traveltime[T.4]          0.6905      0.193      3.570      0.000       0.311       1.070\n",
       "studytime[T.2]          -0.2419      0.068     -3.557      0.000      -0.375      -0.108\n",
       "studytime[T.3]          -0.3243      0.094     -3.436      0.001      -0.510      -0.139\n",
       "studytime[T.4]          -0.3871      0.132     -2.930      0.003      -0.646      -0.128\n",
       "paid[T.yes]              0.2778      0.079      3.519      0.000       0.123       0.433\n",
       "nursery[T.yes]          -0.1761      0.072     -2.461      0.014      -0.317      -0.036\n",
       "famrel[T.2]             -0.2902      0.213     -1.361      0.174      -0.709       0.128\n",
       "famrel[T.3]             -0.2921      0.182     -1.608      0.108      -0.649       0.064\n",
       "famrel[T.4]             -0.5176      0.172     -3.005      0.003      -0.856      -0.180\n",
       "famrel[T.5]             -0.7569      0.175     -4.314      0.000      -1.101      -0.413\n",
       "goout[T.2]              -0.1904      0.439     -0.433      0.665      -1.053       0.672\n",
       "goout[T.3]              -0.2495      0.852     -0.293      0.770      -1.922       1.423\n",
       "goout[T.4]              -0.1777      1.268     -0.140      0.889      -2.666       2.310\n",
       "goout[T.5]              -0.1155      1.697     -0.068      0.946      -3.445       3.214\n",
       "health[T.2]              0.1798      0.114      1.577      0.115      -0.044       0.403\n",
       "health[T.3]              0.1912      0.101      1.889      0.059      -0.007       0.390\n",
       "health[T.4]              0.2528      0.105      2.404      0.016       0.046       0.459\n",
       "health[T.5]              0.3066      0.093      3.308      0.001       0.125       0.489\n",
       "Course[T.Por]            0.1268      0.066      1.927      0.054      -0.002       0.256\n",
       "age                      0.0720      0.025      2.875      0.004       0.023       0.121\n",
       "absences                 0.0198      0.005      4.197      0.000       0.011       0.029\n",
       "index                    1.3718      1.687      0.813      0.416      -1.938       4.682\n",
       "==============================================================================\n",
       "Omnibus:                       21.226   Durbin-Watson:                   1.897\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               21.968\n",
       "Skew:                           0.346   Prob(JB):                     1.70e-05\n",
       "Kurtosis:                       3.163   Cond. No.                     1.87e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.87e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_aft_removing.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This new regression gives us the **correlation between the remaining 18 features in the dataset that seemed to have a strong impact on DWI**. Normally, more features will help increse the R-squared value. Althogh our R-squared went down by 0.012 after removing the features, we must consider that we removed close to half the features in the inital dataset (15 features) yet still mainained a similar R-squared value. **This indicates that removing those 15 variables actually made a more precise model.** We will use this new multi-linear regression model to predict the DWI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual DWI vs. Predicted DWI (after removing insignificant variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/plt_actual_pred_dwi_aft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TEXT ABOUT THE ACTUAL VS. PREDICTED GRAPH HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/plt_resid_actual_pred_dwi_aft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TEXT ABOUT THE RESIDUAL GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithms</th>\n",
       "      <th>weekday_drinking_accuracy</th>\n",
       "      <th>weekend_drinking_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.542857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.580952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Reg</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.470000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      algorithms  weekday_drinking_accuracy  weekend_drinking_accuracy\n",
       "0  Decision Tree                   0.666667                   0.542857\n",
       "1  Random Forest                   0.800000                   0.761905\n",
       "2            SVM                   0.628571                   0.580952\n",
       "3   Logistic Reg                   0.660000                   0.470000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Approach\n",
    "> We used machine learning approaches to predict the level of students' alcohol consumption on Weekdays and on Weekends using other features that reflect students' family, social, and academic environments. Since we are trying to predict categorical variables (level of students' alcohol consumption from 1 to 5 with 1 being very low (less and 5 being very high); we decided to use decision tree classifier, random forest classifier, and support vector classifier as our primary machine learning algorithms with multi-class logistic regression as a benchmark for comparison. We believe that decision tree based algorithms like decision tree classifier & random forests  classifier are best suited for this type of classification problem since a significant portion of our independent/expanatory variables are categorical in nature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Learning \n",
    "> Decision tree learning is one of the most predictive modeling approaches in data mining and machine learning. It is a simple and widely used classification technique. In essence, the decision tree organizes a series of test questions and conditions just like a tree structure. Starting from the root node, we implemented the test condition, and thereby seperated the data into two groups. From there, we apply different test condition on the different subset until the data is entirely classified into smaller subsets. One of the major adventages of a decision tree model is that it is very intuitive and relatively straight-forward, which we thought might potentially render a more accurate model. In addition, nonlinear relationships between parameter do not affect tree performance. Nonetheless, without proper pruning and feature engineering, the tree tends to overfit the training data, which would drastically affect the predicted outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier \n",
    "> The main idea behind a random forest classifier algorithm is to combine many decision trees into a single model. This model is leveraging the idea of sample mean, which suggests that individually, predictions made by decision trees may not be accurate, but combined together, the predictions will be closer to the mark on average.\n",
    "The random forest algorithm pools in predictions and incorporate much more knowledge and accuracy than a single Tree Classifier. It's main advantage is that the model does not get swayed by a single anomalous data source, hence, the predcition will be more accurate and normally distributed, which we hope will produce a more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation & Preprocessing\n",
    "> To prepare our dataset for machine learning algorithms, we first converted all the categorical dependent variables into binary dummie variables using the 'get_dummies()' function (since all of the categorical features contain fewer than 20 unique values). In addition, we included normalization using 'MinMaxScaler' as part of the pipeline and used the 'sklearn.preprocessing' package's 'train_test_split()' function to split our data into training and testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict student alcohol consumption on weekdays\n",
    "The graphs below represents actual level of alcohol consumption versus predicted level of alcohol consumption on Weekdays. We can clearly see that most data points are clustered around the lower left corner and that our machine learning models performed relatively well on predicting lower levels of alcohol consumptions. In particular, the random forests classifer model yielded the best results with most data points on the 45 degree line (actual = predicted). Furthermore, we discovered that a single decision tree by itself is relatively unstable and inaccurate. On the other hand, the logistic regression (benchmark) performed on par with decision tree classifier and support vector classifier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classification | Random Forest Classification | Support Vector Classification | Logistic Regression (Benchmark)\n",
    "---- | ---- | ---- | ----\n",
    "![Using decision tree to predict weekday alcohol consumption](img/decision_tree_weekday.png) | ![Using random forest to predict weekday alcohol consumption](img/random_forest_weekday.png) | ![Using SVC to predict weekday alcohol consumption](img/svc_weekday.png) | ![Using logistic regression to predict weekday alcohol consumption](img/logistic_regression_weekday.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict student alcohol consumption on weekends\n",
    "> The graphs below represents actual level of alcohol consumption versus predicted level of alcohol consumption on Weekends. Unlike the previous actual v.s. prediction graph for Weekday alcohol consumption levels, the data points in the graphs below are spread out more evenly across the diagonal line meaning that the overall levels of students' alcohol consumption are higher on Weekends compare to Weekdays. In addition, we discovered that the random forests classifier consistently out performs the benchmark and the other machine learning classifiers. While decision tree classifier, and the support vector classifer performed similarly with almost the same amount of data points on the 45 degree line (actual = predicted). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classification | Random Forest Classification | Support Vector Classification | Logistic Regression (Benchmark)\n",
    "---- | ---- | ---- | ----\n",
    "![Using decision tree to predict weekend alcohol consumption](img/decision_tree_weekend.png) | ![Using random forest to predict weekend alcohol consumption](img/random_forest_weekend.png) | ![Using SVC to predict weekend alcohol consumption](img/svc_weekend.png) | ![Using logistic regression to predict weekend alcohol consumption](img/logistic_regression_weekend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "1. There is a big limitation to the data we gathered from kaggle: the data came from a survey. As such, there may be some students who were not completely honest with their responsese, and we have no way of cross-checking whether the survey informoation truly matches with the students' real data.\n",
    "\n",
    "2. We attemted to apply forward/backward selection methods on this dataset, but were unable to perform such calculations on our machines due to the number of features the dataset required the feature selection methods to compute. When we attempted to run these feature selection methods on our machines, they ran indefinitely. As a result, we had to resort to lower level means of identifying and removing insignificant variables from the dataset, such as using the P-value scores and viewing the correlation indexes between continuous variables.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: \n",
    "- http://www.healthtalk.org/young-peoples-experiences/drugs-and-alcohol/alcohol-and-social-life\n",
    "- https://www.kaggle.com/uciml/student-alcohol-consumption"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
